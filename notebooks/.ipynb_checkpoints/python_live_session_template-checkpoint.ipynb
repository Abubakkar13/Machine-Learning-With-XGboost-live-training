{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6Ijg5wUCTQYG"
   },
   "source": [
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/datacamp/python-live-training-template/blob/master/assets/datacamp.svg?raw=True\" alt = \"DataCamp icon\" width=\"50%\">\n",
    "</p>\n",
    "<br><br>\n",
    "\n",
    "# **Machine Learning with XGboost**\n",
    "\n",
    "Welcome to this hands-on training where we will learn how to use XGBoost to create powerful prediction models using gradient boosting. Using Jupyter Notebooks you'll learn how to efficiently create, evaluate, and tune XGBoost models. This session will run for three hours, allowing you time to really immerse yourself in the subject, and includes short breaks and opportunities to ask the expert questions throughout the training. \n",
    "\n",
    "You will learn how to:\n",
    "\n",
    "- How to instantiate and customize XGBoost models\n",
    "- How to use XGBoost's DMatrix to optimize performance\n",
    "- How to evaluate models in XGBoost using the right metrics\n",
    "- How to tune parameters in XGBoost to achieve the best results\n",
    "- How to visualize trees in XGBoost to analyze feature importance\n",
    "\n",
    "\n",
    "## **The Dataset**\n",
    "\n",
    "The dataset to be used in this webinar is a CSV file named `hotel_bookings_clean.csv`, which contains data on hotel bookings. \n",
    "\n",
    "### Acknowledgements\n",
    "The dataset was downloaded on [Kaggle](https://www.kaggle.com/jessemostipak/hotel-booking-demand/). The data is originally from an article called [Hotel booking demand datasets](https://www.sciencedirect.com/science/article/pii/S2352340918315191) by Nuno Antonio, Ana de Almeida, and Luis Nunes. It was then cleaned by Thomas Mock and Antoine Bichat for [#TidyTuesday during the week of February 11th, 2020](https://github.com/rfordatascience/tidytuesday/blob/master/data/2020/2020-02-11/readme.md). For the purposes of this live training, it was further pre-processed to have cleaner ready-to-use features (e.g., dropping of irrelevant columns, one-hot-encoding). The dataset has the following [license](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "### Data Dictionary\n",
    "\n",
    "It contains the **53 columns**:\n",
    "\n",
    "_For binary variables: `1` = true and `0` = false._\n",
    "\n",
    "- `is_cancelled`: binary variable indicating whether a booking was canceled\n",
    "- `lead time`: Number of days bettween booking data and arrival date\n",
    "- `arrival_date_week_number`, `arrival_date_day_of_month`, `arrival_date_month`: Week number, day date, and month number of arrival date. \n",
    "- `stays_in_weekend_nights`, `stays_in_week_nights`: Number of weekend nights (Saturday and Sunday) and week nights (Monday to Friday) the guest booked\n",
    "- `adults`,`children`,`babies`: Number of adults, children, babies booked for the stay\n",
    "- `is_repeated_guest`: binary variable indicating whether the customer was a repeat guest \n",
    "- `previous_cancellations`: Number of prior bookings that were cancelled by the customer\n",
    "- `previous_bookings_not_canceled`: Number of prior bookings that were not cancelled by the customer\n",
    "- `required_car_parking_spaces`: Number of parking spaces needed by the customer\n",
    "- `total_of_special_requests`: Number of special requests made by the customer\n",
    "- `avg_daily_rate`: Average Daily Rate as defined by dividing the sum of all lodging transactions by the total number of staying nights\n",
    "- `booked_by_company`: binary variable indicating whether the booking was booked by a company               \n",
    "- `booked_by_agent`: binary variable indicating whether the booking was booked by an agent \n",
    "- `hotel_city Hotel`: binary variable indicating whether the booked hotel is a \"City Hotel\"\n",
    "- `hotel_Resort Hotel`: binary variable indicating whether the booked hotel is a \"Resort Hotel\"\n",
    "- `meal_BB`: binary variable indicating whether a bed & breakfast meal was booked \n",
    "- `meal_HB`: binary variable indicating whether a half board meal was booked\n",
    "- `meal_FB`: binary variable indicating whether a full board meal was booked \n",
    "- `meal_No meal`:binary variable indicating whether there was no meal package booked \n",
    "- `market_segment_Aviation`, `market_segment_Complementary`, `market_segment_Corporate`, `market_segment_Direct`, `market_segment_Groups`, `market_segment_Offline TA/TO`, `market_segment_Online TA`, `market_segment_Undefined`: Indicates market segment designation with a value of `1`. \"TA\"= travel agent, \"TO\"= tour operators.\n",
    "- `distribution_channel_Corporate`, `distribution_channel_Direct`, `distribution_channel_GDS`, `distribution_channel_TA/TO`, `distribution_channel_Undefined`: indicates booking distribution channel with a value of `1`. \"TA\"= travel agent, \"TO\"= tour operators, \"GDS\" = Global Distribution System.\n",
    "- `reserved_room_type_A`,`reserved_room_type_B`, `reserved_room_type_C`,`reserved_room_type_D`, `reserved_room_type_E`, `reserved_room_type_F`, `reserved_room_type_G`, `reserved_room_type_H`, `reserved_room_type_L`: indicates code of room type reserved with a value of `1`. Code is presented instead of designation for anonymity reasons\n",
    "- `deposit_type_No Deposit`: binary variable indicating whether a deposit was made\n",
    "- `deposit_type_Non Refund`: binary variable indicating whether a deposit was made in the value of the total stay cost\n",
    "- `deposit_type_Refundable`: binary variable indicating whether a deposit was made with a value under the total stay cost \n",
    "- `customer_type_Contract`: binary variable indicating whether the booking has an allotment or other type of contract associated to it \n",
    "- `customer_type_Group`: binary variable indicating whether the booking is associated to a group \n",
    "- `customer_type_Transient`: binary variable indicating whether the booking is not part of a group or contract, and is not associated to other transient booking\n",
    "- `customer_type_Transient-Party`: binary variable indicating whether the booking is transient, but is associated to at least other transient booking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BMYfcKeDY85K"
   },
   "source": [
    "## **1. Getting to know our data**\n",
    "\n",
    "Let's get to know our columns and split our data into features and labels!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EMQfyC7GUNhT"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import xgboost as xgb # XGBoost typically uses the alias \"xgb\"\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 479
    },
    "colab_type": "code",
    "id": "l8t_EwRNZPLB",
    "outputId": "36a85c6f-f2ae-44e0-ac01-fc55462bc616"
   },
   "outputs": [],
   "source": [
    "# Read in the dataset\n",
    "bookings = pd.read_csv('https://raw.githubusercontent.com/datacamp/Machine-Learning-With-XGboost-live-training/master/data/hotel_bookings_clean.csv')\n",
    "\n",
    "# List out our columns\n",
    "bookings.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we have 52 columns with 119,210 rows. All the datatypes are numeric and ready for use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IAfz_jiu0NjN"
   },
   "outputs": [],
   "source": [
    "# Take a closer look at column distributions\n",
    "bookings.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot cancellation counts to visualize proportion of not cancelled and cancelled\n",
    "bookings['is_canceled'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember for our binary variables, like `is_canceled`, `1` = true and `0` = false."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's get an exact percentage of not cancelled and cancelled\n",
    "(bookings['is_canceled'].value_counts()/bookings['is_canceled'].count())*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which features are most correlated to cancelations?\n",
    "\n",
    "#### Correlation Coefficient\n",
    "- Quantifies the linear relationship between two variables\n",
    "- Number between -1 and 1\n",
    "- Magnitude corresponds to strength of relationship\n",
    "- Sign (+ or -) corresponds to direction of relationship\n",
    "- Most common way to calculate: **Pearson product-moment correlation coefficient**\n",
    "\n",
    "![Plots displaying different levels of correlation](https://github.com/datacamp/Machine-Learning-With-XGboost-live-training/blob/master/assets/correlation.png?raw=true)\n",
    "\n",
    "We can use `pandas`'s [function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html) `DataFrame.corr()` which returns a correlation matrix using the Pearson correlation coefficient as default. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which features are most correlated with `is_cancelled`?\n",
    "correlation = bookings.corr()['is_canceled'].sort_values(ascending=False)\n",
    "correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be cautious, correlation does not equal feature importance! Correlation may not neccesarily help differentiate classes. Also, the Pearson coefficient only considers **linear** relationships and some of these variables are binary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting data\n",
    "Let's split our label and features so we can get to building models! The first column is our label `is_cancelled`, the rest are features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define X and y\n",
    "X, y = bookings.iloc[:,1:], bookings.iloc[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Your First XGBoost Classifier \n",
    "\n",
    "XGBoost has a  [scikit-learn API](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn), which is useful if you want to use different scikit-learn classes and methods on an XGBoost model (e.g.,`predict()`, `fit()`). \n",
    "\n",
    "In this section, we'll try the API out with the `xgboost.XGBClassifier()` class and get a baseline accuracy for the rest of our work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train and test split using sklearn\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=.33, random_state=123)\n",
    "\n",
    "# Instatiate a XGBClassifier with gbtree as the booster\n",
    "xgb_clf = xgb.XGBClassifier(booster=\"gbtree\", random_state=123))\n",
    "\n",
    "# Inspect the parameters\n",
    "xgb_clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the default objective for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set some parameters so it doesn't run too long\n",
    "### max depth = 6: Maximum tree depth for base learners.\n",
    "### n_estimators = 10: Number of gradient boosted trees. (aka num_boost_rounds)\n",
    "xgb_clf.set_params(max_depth=6, n_estimators=10)\n",
    "\n",
    "# Fit it to the training set\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "preds = xgb_clf.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy\n",
    "## Accuracy = correctly predicted data points / all the data points \n",
    "accuracy = float(np.sum(preds==y_test))/y_test.shape[0]\n",
    "\n",
    "# Print the baseline accuracy\n",
    "print(\"baseline accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing your tree\n",
    "\n",
    "`XGBoost` has two handy visualization functions for interpreting results:\n",
    "- `plot_importance()`\n",
    "- `plot_tree()`\n",
    "\n",
    "#### plot_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot importance\n",
    "xgb.plot_importance(xgb_clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Try plotting importance again with a nicer width/height of 10/8\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "xgb.plot_importance(xgb_clf, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is importance calculated? Here's an excerpt from the [documentation](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.plotting):\n",
    "\n",
    "> **importance_type (str, default \"weight\") –**\n",
    "> How the importance is calculated: either “weight”, “gain”, or “cover”\n",
    "> - ”weight” is the number of times a feature appears in a tree\n",
    "> - ”gain” is the average gain of splits which use the feature\n",
    "> - ”cover” is the average coverage of splits which use the feature where coverage is defined as the number of samples affected by the split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adel, my plots won't plot unless I redefine the axes. Is there any way to avoid this?\n",
    "fig, ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "# Plot gain instead of weight\n",
    "xgb.plot_importance(xgb_clf, ax=ax, importance_type=\"gain\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "\n",
    "# Plot the first tree\n",
    "xgb.plot_tree(xgb_clf, num_trees=0, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out a higher resolution version of the tree [here](https://github.com/datacamp/Machine-Learning-With-XGboost-live-training/blob/master/assets/tree1.png?raw=true)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "\n",
    "# Plot the last tree\n",
    "## Make the direction left to right instead of top to bottom\n",
    "xgb.plot_tree(xgb_clf, num_trees=9, ax=ax, rankdir='LR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out a higher resolution version of the tree [here](https://github.com/datacamp/Machine-Learning-With-XGboost-live-training/blob/master/assets/tree2.png?raw=true)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Cross Validation in XGBoost\n",
    "\n",
    "Cross validation is considered best practice for assessing a model's performance. We can use `xgboost.cv()` to efficiently run cross validation on XGBoost models. This method is part of XGBoost's core library and **not** part of XGBoost's scikit-learn API from earlier. \n",
    "\n",
    "![K-Fold Cross Validation](https://upload.wikimedia.org/wikipedia/commons/b/b5/K-fold_cross_validation_EN.svg)\n",
    "\n",
    "_Image attribution: Gufosowa / CC BY-SA (https://creativecommons.org/licenses/by-sa/4.0), [Source Link](https://commons.wikimedia.org/wiki/File:K-fold_cross_validation_EN.svg)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to DMartix\n",
    "\n",
    "In order to use `xgboost.cv()`, we need to convert our dataframe into a `DMatrix`. This will optimize for both memory efficiency and training speed. One of the biggest advantages of using the non-scikit-learn compatible functions is that you can use `DMatrix`. Conversion is simple!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert X and y into a DMatrix\n",
    "bookings_dmatrix = xgb.DMatrix(data=X,label=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross validation with xgb.cv\n",
    "\n",
    "Note: Scikit-learn uses `n_estimator` to refer to the number of boosting rounds or number of gradient boosted trees. In XGBoost, it's refer to as `num_boost_rounds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define booster parameters using a dictionary\n",
    "## Default: objective=\"binary:logistic\" and the max_depth=6\n",
    "params = {\"objective\":\"binary:logistic\", 'max_depth': 6}\n",
    "\n",
    "# Instantiate a CV object\n",
    "## Default: 3 folds and 10 boosting rounds\n",
    "xgb_cv = xgb.cv(dtrain=bookings_dmatrix, params=params, nfold=3, num_boost_round=10, seed=123)\n",
    "\n",
    "# Inspect the results: how are they stored?\n",
    "xgb_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Results in XGBoost\n",
    "\n",
    "`error` as defined by [XGBoost documentation](https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters):\n",
    "> Binary classification error rate. It is calculated as `#(wrong cases)/#(all cases)`. For the predictions, the evaluation will regard the instances with prediction value larger than 0.5 as positive instances, and the others as negative instances.\n",
    "\n",
    "So we need to subtract the last boosting round's `test-error-mean` from 1 to get the accuracy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the result from last boosting round\n",
    "results = xgb_cv.tail(1)\n",
    "\n",
    "# Caclulate accuracy\n",
    "accuracy= 1 - xgb_cv[\"test-error-mean\"].iloc[-1]\n",
    "\n",
    "# Print the baseline accuracy\n",
    "print(\"baseline accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very close to our `xgboost.XGBClassifier()` baseline of `0.8145` which used the same booster parameters. This helps validate its performance. Let's now look into improving performace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More trees with early stopping\n",
    "\n",
    "We've been using 10 trees (aka `num_boost_rounds`). Let's add more trees (40 instead of 10), but make sure to add **early stopping**. \n",
    "\n",
    "Early stopping works by testing the model after every boosting round against the holdout set and if the holdout metric (error in our case) has not improved after a given number of rounds (defined by `early_stopping_rounds`), then any additional boosting rounds are stopped. If the model continuously improves up to `num_boost_round`, then early stopping does not occur.\n",
    "\n",
    "This helps automatically select the nuumber of boosting rounds and minimize unnecessary training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the number of trees to 40 and set the early stopping rounds to 5\n",
    "xgb_cv = xgb.cv(dtrain=bookings_dmatrix, params=params, nfold=3,\n",
    "                    num_boost_round=40,early_stopping_rounds=5, seed=123)\n",
    "\n",
    "# Caclulate accuracy\n",
    "accuracy= 1 - xgb_cv[\"test-error-mean\"].iloc[-1]\n",
    "\n",
    "# Print the accuracy\n",
    "print(\"accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Did early stopping happen?\n",
    "xgb_cv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've improved the results by increasing the number of boosted trees, but there are more parameters we can play with!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Digging into Parameters\n",
    "\n",
    "Along with setting the number of boosting rounds and early stopping rounds, there are many other parameters. We can see this in the documentation for [Parameters for tree booster](https://xgboost.readthedocs.io/en/latest/parameter.html#parameters-for-tree-booster). _Adel I'll actually click on this link._\n",
    "\n",
    "In this section, we'll take a look at several important parameters and get an understanding of what they do.\n",
    "\n",
    "For the purpose of this session, we'll use `XGBClassifier()` with 25 boosting rounds to avoid long training times. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Instantiate the XGBClassifier with 25 boosting rounds\n",
    "xgb_clf = xgb.XGBClassifier(n_estimators=25, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Max depth\n",
    "\n",
    "_From XGBoost docs:_\n",
    "> Maximum depth of a tree. Increasing this value will make the model more complex and more likely to overfit.\n",
    "\n",
    "Let's see what happens when we increase the max_depth from 6 to 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set max_depth to 10\n",
    "xgb_clf.set_params(max_depth=10)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "preds = xgb_clf.predict(X_test)\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### colsample_bytree\n",
    "\n",
    "_From XGBoost docs:_\n",
    "\n",
    "> The subsample ratio of columns when constructing each tree. Subsampling occurs once for every tree constructed.\n",
    "\n",
    "This adds randomness, making the model more robust to noise. The default is 1, let's try a smaller value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set colsample_bytree to 0.5 and print accuracy\n",
    "xgb_clf.set_params(colsample_bytree=0.5)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "preds = xgb_clf.predict(X_test)\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar parameters:\n",
    "\n",
    "_From XGBoost docs:_\n",
    "\n",
    "> `colsample_bylevel` is the subsample ratio of columns for each level. Subsampling occurs once for every new depth level reached in a tree. Columns are subsampled from the set of columns chosen for the current tree.\n",
    "\n",
    "> `colsample_bynode` is the subsample ratio of columns for each node (split). Subsampling occurs once every time a new split is evaluated. Columns are subsampled from the set of columns chosen for the current level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### subsample\n",
    "\n",
    "_From XGBoost docs:_\n",
    "\n",
    "> - Subsample ratio of the training instances. Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees and this will prevent overfitting. \n",
    "> - Subsampling will occur once in every boosting iteration.\n",
    "> - range: (0,1]\n",
    "\n",
    "Default is 1, let's try 0.75."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set subsample to 0.75 and print accuracy\n",
    "xgb_clf.set_params(subsample=0.75)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "preds = xgb_clf.predict(X_test)\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### min_child_weight\n",
    "\n",
    "_From XGBoost docs:_\n",
    "\n",
    "> - Minimum sum of instance weight (hessian) needed in a child. If the tree partition step results in a leaf node with the sum of instance weight less than min_child_weight, then the building process will give up further partitioning. \n",
    "> - The larger min_child_weight is, the more conservative the algorithm will be.\n",
    "> - range: [0,∞]\n",
    "\n",
    "\n",
    "Let's increase `min_child_weight`. This will decrease model complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set min_child_weight to 1.5 and print accuracy\n",
    "xgb_clf.set_params(min_child_weight=1.5)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "preds = xgb_clf.predict(X_test)\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gamma\n",
    "\n",
    "_From XGBoost docs:_\n",
    "> - Minimum loss reduction required to make a further partition on a leaf node of the tree. The larger gamma is, the more conservative the algorithm will be.\n",
    "> - range: [0,∞]\n",
    "\n",
    "Default is 0. Let's increase it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set gamma to .25 and print accuracy\n",
    "xgb_clf.set_params(gamma=0.25)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "preds = xgb_clf.predict(X_test)\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## alpha\n",
    "\n",
    "_From XGBoost docs:_\n",
    "> L1 regularization term on weights. Increasing this value will make model more conservative.\n",
    "\n",
    "Default is 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set alpha to .1 and print accuracy\n",
    "xgb_clf.set_params(reg_alpha=0.01)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "preds = xgb_clf.predict(X_test)\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 is also available with `reg_lambda`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate (aka eta)\n",
    "\n",
    "_From XGBoost docs:_\n",
    "> - Step size shrinkage used in update to prevents overfitting. After each boosting step, we can directly get the weights of new features, and eta shrinks the feature weights to make the boosting process more conservative.\n",
    "> - range: [0,1]\n",
    "\n",
    "Default is 0.30. What happens if we decrease it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set learning rate to .1 and print accuracy\n",
    "xgb_clf.set_params(learning_rate=0.1)\n",
    "\n",
    "# Fit the classifier to the training set\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict the labels of the test set\n",
    "preds = xgb_clf.predict(X_test)\n",
    "\n",
    "# Compute the accuracy\n",
    "accuracy_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model parameters\n",
    "xgb_clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a lot of possible parameters combinations. We can't manually tune and pick them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Hyperparameter tuning with Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search and random search are the most popular methods for hyperparameter tuning. However, grid search can get computationally expensive if you are searching over a large hyperparameter space and dealing with multiple hyperparameters. In this last section, this is why we'll use random search because not all hyperparemeter values are tried. In random search, a fixed number of hyperparameter settings is sampled from specified probability distributions.\n",
    "\n",
    "XGBoost doesn't have a built-in gridsearch function, so we need to use `scikit-learn`'s [RandomizedSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html). This means we'll have to use `XGBClassifier()` because it's `scikit-learn` compatible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'xgb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-105ba083f865>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# instantiate a new XGBClassifier()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m## Let's keep at n_estimator to 20 to keep computation time low(er)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mxgb_clf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Instantiate RandomizedSearchCV()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'xgb' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "# Create a parameter grid for three parameters\n",
    "## max_depth: values from 2 to 12\n",
    "## alpha: values 0, .001, .01, .1\n",
    "## subsample: values 0.25,0.5,0.75, 1\n",
    "## min_child_weight: values 1, 1.5, 2\n",
    "rs_param_grid = {\n",
    "    'max_depth': list((range(2,12))),\n",
    "    'alpha': [0,0.001, 0.01,0.1,1],\n",
    "    'subsample': [0.5,0.75,1],\n",
    "    'min_child_weight': [1,1.5,2]\n",
    "}\n",
    "\n",
    "# instantiate a new XGBClassifier()\n",
    "## Let's keep at n_estimator to 20 to keep computation time low(er)\n",
    "xgb_clf = xgb.XGBClassifier(n_estimators=20, random_state=123)\n",
    "\n",
    "# Instantiate RandomizedSearchCV()\n",
    "## For random search, we use param_distributions instead of params\n",
    "## n_iter: the number of parameter settings that are tried\n",
    "## verbose: get more output on what's being computers\n",
    "xgb_rs = RandomizedSearchCV(estimator=xgb_clf,param_distributions=rs_param_grid, \n",
    "                                cv=3, n_iter=5, verbose=2, random_state=123)\n",
    "\n",
    "# Train the model on the training set\n",
    "xgb_rs.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters and lowest RMSE\n",
    "print(\"Best parameters found: \", xgb_rs.best_params_)\n",
    "print(\"Best accuracy found: \", xgb_rs.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take home assignment\n",
    "\n",
    "The above code takes about ~3 minutes to run and reaches an accuracy of ~0.8403 on the training data (`X_train`,`y_train`).\n",
    "\n",
    "With less time restrictions at home, what is the highest accuracy you can reach on the test set (`X_test`,`y_test`)? Make sure to play around with the parameters and their values in `rs_param_grid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "python_live_session_template.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
